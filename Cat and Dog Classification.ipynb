{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":196452,"sourceType":"datasetVersion","datasetId":84954}],"dockerImageVersionId":31089,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"source":"<a href=\"https://www.kaggle.com/code/nazaninmahmoudy/dogs-and-cats-classification-transfer-learning?scriptVersionId=253268313\" target=\"_blank\"><img align=\"left\" alt=\"Kaggle\" title=\"Open in Kaggle\" src=\"https://kaggle.com/static/images/open-in-kaggle.svg\"></a>","metadata":{},"cell_type":"markdown"},{"cell_type":"markdown","source":"# ðŸ¾  Cat vs Dog Classifier With Transfer Learning (2025)","metadata":{}},{"cell_type":"markdown","source":"#### ðŸ“Œ **Project Overview**\n\nThis project focuses on binary image classification using deep learning and **transfer learning** techniques. The objective is to classify images as either Cat or Dog using a pre-trained MobileNetV2 model. The **goal** is to demonstrate how transfer learning can achieve **high accuracy** even with a limited dataset, while also integrating essential computer vision techniques like image preprocessing and evaluation metrics.\n","metadata":{}},{"cell_type":"markdown","source":"#### ðŸ–¼ï¸ **Dataset**\n\nThe dataset contains 10,000 labeled images, evenly distributed between cats and dogs. The images are preprocessed with resizing and normalizationto improve generalization. The data was split into training (4000), validation (1000), and test (5000) sets.\n\n","metadata":{}},{"cell_type":"markdown","source":"#### **âš™ï¸ Model & Approach**\n\nThe model architecture is based on MobileNetV2 from TensorFlow Keras applications, pre-trained on ImageNet. This base model was frozen during training, and custom dense layers were added on top for binary classification.\n\nðŸ”§ **Model Components**:\n\nâœ… Pretrained MobileNetV2 (feature extractor)\n\nâœ… GlobalAveragePooling2D\n\nâœ… Dense layer with ReLU activation\n\nâœ… Dropout (to reduce overfitting)\n\nâœ… Dense output layer with softmax activation (2 classes)\n\nðŸ“¦ Loss Function: sparse_categorical_crossentropy                   \nðŸš€ Optimizer: Adam                       \nðŸŽ¯ Metrics: Accuracy                 ","metadata":{}},{"cell_type":"markdown","source":"#### **ðŸ“Š Dataset**\n\nThe dataset consists of a total of 25,000 labeled images of cats and dogs:\n\ntrain/cats: 10,000 images\n\ntrain/dogs: 10,000 images\n\ntest/cats: 2,500 images\n\ntest/dogs: 2,500 images\n\nIn this project, a subset of 5,000 images (2,500 per class) from the test folder was used as the final test set.\nThe training data (20,000 images total) was further split into:\n\nTraining set: 4,000 images (stratified sample)\n\nValidation set: 1,000 images\n\nAll images were resized to **224x224**, normalized, and augmented during training to improve generalization.\n\n","metadata":{}},{"cell_type":"markdown","source":"#### ðŸ“ˆ Evaluation Metrics\n\nThe model achieved outstanding performance across all standard classification metrics. When evaluated on the official test set consisting of 5,000 images (2,500 cats and 2,500 dogs), it reached an overall accuracy of 97%, with balanced precision, recall, and F1-scores of 0.97 for both classes. \nFurthermore, to assess its generalization ability, the model was also tested on **unseen external images** (images not part of the dataset, fetched separately from the web). Impressively, it maintained a high prediction accuracy of around 99% on these new inputs. This demonstrates the robustness and transferability of features learned through the MobileNetV2 backbone.\nThe success of this model, despite being **trained on a relatively small portion** of the available training data, further underscores the effectiveness of **transfer learning** in image classification tasks.\n\n","metadata":{}},{"cell_type":"markdown","source":"#### **ðŸ“ Dataset**\n\nKaggle Dataset URL: https://www.kaggle.com/c/dogs-vs-cats\n\n","metadata":{}},{"cell_type":"markdown","source":"#### **ðŸ‘©â€ðŸ’» Author & Contact**\n\nðŸ“Œ Project by: Nazanin Mahmoudy, 2025                      \nðŸ“§ Email: Nazaninmahmoudy@gmail.com                       \nðŸ”— GitHub: https://github.com/Nazaninmahmoudi                   \nðŸ”— Kaggle: https://www.kaggle.com/nazaninmahmoudy                     ","metadata":{}},{"cell_type":"markdown","source":"## Importing Libraries","metadata":{}},{"cell_type":"code","source":"import os\nimport cv2\nimport urllib.request\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport tensorflow as tf\nimport random\nimport cv2\nfrom sklearn.metrics import classification_report, confusion_matrix\nfrom sklearn.metrics import roc_curve, auc, precision_recall_curve, average_precision_score, confusion_matrix\n","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-07-28T07:36:21.744764Z","iopub.execute_input":"2025-07-28T07:36:21.745446Z","iopub.status.idle":"2025-07-28T07:36:42.099087Z","shell.execute_reply.started":"2025-07-28T07:36:21.745417Z","shell.execute_reply":"2025-07-28T07:36:42.09798Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"###  Dataset Loading & Overview","metadata":{}},{"cell_type":"code","source":"dataset_dir = '/kaggle/input/dogs-vs-cats'","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-28T07:36:42.100627Z","iopub.execute_input":"2025-07-28T07:36:42.101284Z","iopub.status.idle":"2025-07-28T07:36:42.105328Z","shell.execute_reply.started":"2025-07-28T07:36:42.101257Z","shell.execute_reply":"2025-07-28T07:36:42.104298Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def count_images_in_folder(folder_path):\n    valid_extensions = ('.jpg', '.jpeg', '.png')\n    return len([f for f in os.listdir(folder_path) if f.lower().endswith(valid_extensions)])\n\nfolders = {\n    'train/cats': os.path.join(dataset_dir, 'train', 'cats'),\n    'train/dogs': os.path.join(dataset_dir, 'train', 'dogs'),\n    'test/cats': os.path.join(dataset_dir, 'test', 'cats'),\n    'test/dogs': os.path.join(dataset_dir, 'test', 'dogs')\n}\n\nfor name, path in folders.items():\n    count = count_images_in_folder(path)\n    print(f\"{name}: {count} images\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-28T07:36:42.106576Z","iopub.execute_input":"2025-07-28T07:36:42.1076Z","iopub.status.idle":"2025-07-28T07:36:42.402515Z","shell.execute_reply.started":"2025-07-28T07:36:42.107449Z","shell.execute_reply":"2025-07-28T07:36:42.401703Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"As we can see, there are 2 Clasess and the number of them is represented ","metadata":{}},{"cell_type":"markdown","source":"## Displaying Random Images","metadata":{}},{"cell_type":"code","source":"def show_random_image(cat_folder, dog_folder, img_size=224):\n   \n    selected_class = random.choice(['cat', 'dog'])\n    folder = cat_folder if selected_class == 'cat' else dog_folder\n\n    valid_exts = ('.jpg', '.jpeg', '.png')\n    image_files = [f for f in os.listdir(folder) if f.lower().endswith(valid_exts)]\n\n    if not image_files:\n        print(f\"No valid images found in {folder}\")\n        return\n\n    random_image = random.choice(image_files)\n    img_path = os.path.join(folder, random_image)\n\n    img = cv2.imread(img_path)\n    if img is None:\n        print(f\"Couldn't read image {img_path}\")\n        return\n\n    img_rgb = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n    img_resized = cv2.resize(img_rgb, (img_size, img_size))\n\n    plt.figure(figsize=(4, 4))\n    plt.imshow(img_resized)\n    plt.title(f\"Random {selected_class.capitalize()} Image\")\n    plt.axis('off')\n    plt.show()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-28T07:36:42.404526Z","iopub.execute_input":"2025-07-28T07:36:42.404862Z","iopub.status.idle":"2025-07-28T07:36:42.415262Z","shell.execute_reply.started":"2025-07-28T07:36:42.404838Z","shell.execute_reply":"2025-07-28T07:36:42.414247Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### Load & Preprocess Images","metadata":{}},{"cell_type":"code","source":"cat_path = os.path.join(dataset_dir, 'train', 'cats')\ndog_path = os.path.join(dataset_dir, 'train', 'dogs')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-28T07:36:42.416238Z","iopub.execute_input":"2025-07-28T07:36:42.416569Z","iopub.status.idle":"2025-07-28T07:36:42.43809Z","shell.execute_reply.started":"2025-07-28T07:36:42.416531Z","shell.execute_reply":"2025-07-28T07:36:42.436986Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"show_random_image(cat_path, dog_path)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-28T07:36:42.439491Z","iopub.execute_input":"2025-07-28T07:36:42.439931Z","iopub.status.idle":"2025-07-28T07:36:42.808406Z","shell.execute_reply.started":"2025-07-28T07:36:42.439905Z","shell.execute_reply":"2025-07-28T07:36:42.806844Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"show_random_image(cat_path, dog_path)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-28T07:36:42.809722Z","iopub.execute_input":"2025-07-28T07:36:42.810172Z","iopub.status.idle":"2025-07-28T07:36:42.998121Z","shell.execute_reply.started":"2025-07-28T07:36:42.810142Z","shell.execute_reply":"2025-07-28T07:36:42.996722Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Data Preprocessing","metadata":{}},{"cell_type":"markdown","source":" In the following part , we prepare the image data for training our deep learning model. We start by setting two key constants: IMG_SIZE, which defines the target size (224x224 pixels) for each image, and MAX_IMAGES_PER_CLASS, which controls how many images to load from each category.\nDeep learning models, especially pretrained architectures like MobileNetV2 that we are using , expect input images to have a fixed shapeâ€”typically 224x224 with 3 color channelsâ€”so resizing is essential to ensure compatibility.","metadata":{}},{"cell_type":"code","source":"IMG_SIZE = 224\nMAX_IMAGES_PER_CLASS = 2500\n\ndef load_images_from_folder(folder, label, max_images=2500):\n    images = []\n    labels = []\n    count = 0\n    for filename in os.listdir(folder):\n        if count >= max_images:\n            break\n        if filename.lower().endswith(('.jpg', '.png', '.jpeg')):\n            path = os.path.join(folder, filename)\n            img = cv2.imread(path)\n            if img is not None:\n                img = cv2.resize(img, (IMG_SIZE, IMG_SIZE))\n                images.append(img)\n                labels.append(label)\n                count += 1\n    return images, labels","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-28T07:36:42.999225Z","iopub.execute_input":"2025-07-28T07:36:42.999567Z","iopub.status.idle":"2025-07-28T07:36:43.008992Z","shell.execute_reply.started":"2025-07-28T07:36:42.999538Z","shell.execute_reply":"2025-07-28T07:36:43.00715Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"cat_train_folder = os.path.join(dataset_dir, 'train', 'cats')\ndog_train_folder = os.path.join(dataset_dir, 'train', 'dogs')\ncat_test_folder = os.path.join(dataset_dir, 'test', 'cats')\ndog_test_folder = os.path.join(dataset_dir, 'test', 'dogs')\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-28T07:36:43.010451Z","iopub.execute_input":"2025-07-28T07:36:43.010861Z","iopub.status.idle":"2025-07-28T07:36:43.035117Z","shell.execute_reply.started":"2025-07-28T07:36:43.010828Z","shell.execute_reply":"2025-07-28T07:36:43.033698Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"To organize our data, we also define four folder paths: training and testing directories for both cat and dog images. These paths will later be used to load and balance our dataset across the two classes.","metadata":{}},{"cell_type":"code","source":"cat_train_images, cat_train_labels = load_images_from_folder(cat_train_folder, 0, MAX_IMAGES_PER_CLASS)\ndog_train_images, dog_train_labels = load_images_from_folder(dog_train_folder, 1, MAX_IMAGES_PER_CLASS)\n\ncat_test_images, cat_test_labels = load_images_from_folder(cat_test_folder, 0)\ndog_test_images, dog_test_labels = load_images_from_folder(dog_test_folder, 1)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-28T07:36:43.03842Z","iopub.execute_input":"2025-07-28T07:36:43.038888Z","iopub.status.idle":"2025-07-28T07:38:06.946486Z","shell.execute_reply.started":"2025-07-28T07:36:43.038863Z","shell.execute_reply":"2025-07-28T07:38:06.945433Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"This block loads cat and dog images from the training and test directories. Each image is resized to a consistent size (224x224) for input into the model and labeled as 0 (cat) or 1 (dog). We limit the number of training images per class but load all test images. This ensures balanced training data and a complete, unbiased test set for evaluation.","metadata":{}},{"cell_type":"markdown","source":"### Scaling Data","metadata":{}},{"cell_type":"code","source":"X_train = np.array(cat_train_images + dog_train_images, dtype=np.float32) / 255.0\nY_train = np.array(cat_train_labels + dog_train_labels)\n\nX_test = np.array(cat_test_images + dog_test_images, dtype=np.float32) / 255.0\nY_test = np.array(cat_test_labels + dog_test_labels)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-28T07:38:06.947824Z","iopub.execute_input":"2025-07-28T07:38:06.948162Z","iopub.status.idle":"2025-07-28T07:38:11.032142Z","shell.execute_reply.started":"2025-07-28T07:38:06.948135Z","shell.execute_reply":"2025-07-28T07:38:11.031056Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"In this part, we are normalizing the image pixel values from their original range of [0, 255] (standard for 8-bit RGB images) to [0.0, 1.0]","metadata":{}},{"cell_type":"markdown","source":"## Splitting Data","metadata":{}},{"cell_type":"code","source":"from sklearn.model_selection import train_test_split\n\nX_train, X_val, Y_train, Y_val = train_test_split(X_train, Y_train, test_size=0.2, random_state=42, stratify=Y_train)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-28T07:38:11.033287Z","iopub.execute_input":"2025-07-28T07:38:11.033716Z","iopub.status.idle":"2025-07-28T07:38:12.317836Z","shell.execute_reply.started":"2025-07-28T07:38:11.03369Z","shell.execute_reply":"2025-07-28T07:38:12.316852Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"By keeping the class distribution balanced between training and validation sets, we ensure fair representation of each class.\nThe validation set lets us track the modelâ€™s performance during training while keeping the test set untouched for unbiased final evaluation.\n","metadata":{}},{"cell_type":"code","source":"print(f\"Train samples: {X_train.shape[0]}\")\nprint(f\"Validation samples: {X_val.shape[0]}\")\nprint(f\"Test samples: {X_test.shape[0]}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-28T07:38:12.318683Z","iopub.execute_input":"2025-07-28T07:38:12.318951Z","iopub.status.idle":"2025-07-28T07:38:12.324753Z","shell.execute_reply.started":"2025-07-28T07:38:12.31893Z","shell.execute_reply":"2025-07-28T07:38:12.323705Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### Model Architecture and Transfer Learning Setup","metadata":{}},{"cell_type":"markdown","source":"#### MobileNetV2","metadata":{}},{"cell_type":"code","source":"base_model = tf.keras.applications.MobileNetV2(\n    input_shape=(224, 224, 3),\n    include_top=False,\n    weights='imagenet'\n)\n\nbase_model.trainable = False\n\nmodel = tf.keras.Sequential([\n    base_model,\n    tf.keras.layers.GlobalAveragePooling2D(),\n    tf.keras.layers.Dense(128, activation='relu'),\n    tf.keras.layers.Dropout(0.3),\n    tf.keras.layers.Dense(2, activation='softmax') \n])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-28T07:38:12.325854Z","iopub.execute_input":"2025-07-28T07:38:12.32623Z","iopub.status.idle":"2025-07-28T07:38:13.864173Z","shell.execute_reply.started":"2025-07-28T07:38:12.326208Z","shell.execute_reply":"2025-07-28T07:38:13.862825Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"The base model is MobileNetV2, a pre-trained convolutional neural network designed for efficient image feature extraction.\n\nBy freezing the base modelâ€™s weights (trainable=False), we prevent altering its learned features during training, which helps speed up training and avoid overfitting given our smaller dataset.\nAfter the base model, we add a global average pooling layer to reduce the spatial dimensions into a single feature vector per image. \nThis is followed by a dense layer with 128 neurons and ReLU activation to learn higher-level features specific to our classification task.\nA dropout layer randomly disables 30% of neurons during training, serving as regularization to further prevent overfitting. \nFinally, the output layer with two neurons and softmax activation produces the predicted probabilities for each class.","metadata":{}},{"cell_type":"markdown","source":"### Compiling the model","metadata":{}},{"cell_type":"code","source":"model.compile(optimizer='adam',\n              loss='sparse_categorical_crossentropy',\n              metrics=['accuracy'])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-28T07:38:13.865206Z","iopub.execute_input":"2025-07-28T07:38:13.865475Z","iopub.status.idle":"2025-07-28T07:38:13.88207Z","shell.execute_reply.started":"2025-07-28T07:38:13.865448Z","shell.execute_reply":"2025-07-28T07:38:13.881118Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"model.summary()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-28T07:38:13.883025Z","iopub.execute_input":"2025-07-28T07:38:13.883343Z","iopub.status.idle":"2025-07-28T07:38:13.91091Z","shell.execute_reply.started":"2025-07-28T07:38:13.883315Z","shell.execute_reply":"2025-07-28T07:38:13.910148Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### Model Summary and Parameters Breakdown","metadata":{}},{"cell_type":"markdown","source":"As we can see , The base MobileNetV2 model outputs feature maps of size 7x7 with 1280 channels, containing about 2.25 million parameters. These parameters are non-trainable because we froze the base modelâ€™s weights.                       \nThe global average pooling layer reduces the spatial dimensions to a 1280-length feature vector per image without adding parameters.                            \nA fully connected dense layer with 128 neurons adds around 164 thousand trainable parameters, enabling the model to learn task-specific features.                             \nthe output dense layer with 2 neurons corresponds to our two classes (cats and dogs), with only 258 trainable parameters.","metadata":{}},{"cell_type":"markdown","source":"## Trainin the model","metadata":{}},{"cell_type":"code","source":"history = model.fit(X_train, Y_train,\n                    epochs=5,\n                    batch_size=16,\n                    validation_data=(X_val, Y_val))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-28T07:38:13.911867Z","iopub.execute_input":"2025-07-28T07:38:13.912139Z","iopub.status.idle":"2025-07-28T07:48:34.005072Z","shell.execute_reply.started":"2025-07-28T07:38:13.912118Z","shell.execute_reply":"2025-07-28T07:48:34.003738Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"During training, the modelâ€™s performance is also evaluated on the validation set after each epoch","metadata":{}},{"cell_type":"markdown","source":"## Model Evaluation","metadata":{}},{"cell_type":"code","source":"test_loss, test_accuracy = model.evaluate(X_test, Y_test)\nprint(f\"Test Loss: {test_loss:.4f}\")\nprint(f\"Test Accuracy: {test_accuracy * 100:.2f}%\\n\")\n\ny_pred_probs = model.predict(X_test)\ny_pred = np.argmax(y_pred_probs, axis=1)\n\nprint(\"Classification Report:\")\nprint(classification_report(Y_test, y_pred, target_names=[\"Cat\", \"Dog\"]))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-28T07:48:34.00613Z","iopub.execute_input":"2025-07-28T07:48:34.006397Z","iopub.status.idle":"2025-07-28T07:53:46.717406Z","shell.execute_reply.started":"2025-07-28T07:48:34.006379Z","shell.execute_reply":"2025-07-28T07:53:46.716256Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### Classification Report Anylizing","metadata":{}},{"cell_type":"markdown","source":"The classification report demonstrates that the model performs remarkably well on the test set of 5,000 images, achieving an overall accuracy of 97%. Both classesâ€”Cat and Dogâ€”exhibit high and balanced metrics. The Cat class achieved a precision of 0.97 and a recall of 0.98, while the Dog class obtained a precision of 0.98 and a recall of 0.97. These strong values yield F1-scores of 0.97 for both classes, indicating a well-balanced trade off between precision and recall. Additionally, the macro and weighted averages are identical across all metrics, suggesting that the model is not biased toward any specific class and maintains consistent performance throughout.","metadata":{}},{"cell_type":"markdown","source":"A key factor contributing to this high performance is the use of **transfer learning**, specifically through a pretrained **MobileNetV2** model. Since MobileNetV2 has been trained on the large and diverse **ImageNet** dataset, it has already learned valuable low-level and mid-level visual features such as edges, textures, and shapes.     \nThis prior knowledge significantly reduces the amount of data and training time required to achieve good results on a new, smaller dataset.\n\nMoreover, **freezing** the convolutional base while training only the classification head speeds up training and acts as a regularizer, helping to reduce overfitting. This is particularly beneficial when working with limited datasets, as it prevents the model from memorizing the training data.","metadata":{}},{"cell_type":"markdown","source":"## Confusion Matrix","metadata":{}},{"cell_type":"code","source":"cm = confusion_matrix(Y_test, y_pred)\nplt.figure(figsize=(6, 5))\nsns.heatmap(cm, annot=True, fmt='d', cmap='Blues',\n            xticklabels=[\"Cat\", \"Dog\"], yticklabels=[\"Cat\", \"Dog\"])\nplt.xlabel(\"Predicted Label\")\nplt.ylabel(\"True Label\")\nplt.title(\"Confusion Matrix\")\nplt.show()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-28T07:53:46.71892Z","iopub.execute_input":"2025-07-28T07:53:46.719328Z","iopub.status.idle":"2025-07-28T07:53:46.923264Z","shell.execute_reply.started":"2025-07-28T07:53:46.719285Z","shell.execute_reply":"2025-07-28T07:53:46.922192Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"As we can see, the confusion matrix demonstrates that the model performs very well in distinguishing between cats and dogs. Out of the total predictions, 2,438 cat images and 2,422 dog images were correctly classified, while only 77 cats were misclassified as dogs and 78 dogs as cats. This indicates that the model has high accuracy and balanced performance across both classes, making it reliable for binary","metadata":{}},{"cell_type":"markdown","source":"## Plot Normalized Confusion Matrix and Print Macro/Micro Scores\n","metadata":{}},{"cell_type":"code","source":"from sklearn.metrics import confusion_matrix, precision_score, recall_score, f1_score\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport numpy as np\n\ndef plot_normalized_confusion_matrix(y_true, y_pred, class_names):\n    cm = confusion_matrix(y_true, y_pred)\n    cm_norm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n\n    plt.figure(figsize=(10, 8))\n    ax = sns.heatmap(cm_norm, annot=True, fmt='.2%', cmap='Blues',\n                     xticklabels=class_names, yticklabels=class_names)\n\n    if cm.shape == (2, 2):\n        tn, fp, fn, tp = cm.ravel()\n        labels = np.array([\n            [f'TN = {tn}', f'FP = {fp}'],\n            [f'FN = {fn}', f'TP = {tp}']\n        ])\n        for i in range(2):\n            for j in range(2):\n                ax.text(j + 0.5, i + 0.5, '\\n' + labels[i, j],\n                        ha='center', va='center', color='black', fontsize=10, weight='bold')\n\n    plt.xlabel('Predicted')\n    plt.ylabel('True')\n    plt.title('Normalized Confusion Matrix with TP, TN, FP, FN')\n    plt.show()\n\n    precision_macro = precision_score(y_true, y_pred, average='macro')\n    recall_macro = recall_score(y_true, y_pred, average='macro')\n    f1_macro = f1_score(y_true, y_pred, average='macro')\n\n    precision_micro = precision_score(y_true, y_pred, average='micro')\n    recall_micro = recall_score(y_true, y_pred, average='micro')\n    f1_micro = f1_score(y_true, y_pred, average='micro')\n\n    print(f\"Macro Precision: {precision_macro:.3f}, Recall: {recall_macro:.3f}, F1-score: {f1_macro:.3f}\")\n    print(f\"Micro Precision: {precision_micro:.3f}, Recall: {recall_micro:.3f}, F1-score: {f1_micro:.3f}\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-28T07:53:46.924687Z","iopub.execute_input":"2025-07-28T07:53:46.925048Z","iopub.status.idle":"2025-07-28T07:53:46.936445Z","shell.execute_reply.started":"2025-07-28T07:53:46.925018Z","shell.execute_reply":"2025-07-28T07:53:46.935312Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"class_names = ['Cat', 'Dog']\n\nplot_normalized_confusion_matrix(Y_test, y_pred, class_names)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-28T07:53:46.93737Z","iopub.execute_input":"2025-07-28T07:53:46.937638Z","iopub.status.idle":"2025-07-28T07:53:47.189105Z","shell.execute_reply.started":"2025-07-28T07:53:46.937614Z","shell.execute_reply":"2025-07-28T07:53:47.188009Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"The evaluation metrics demonstrate a very strong and balanced performance by the model. The macro-averaged precision, recall, and F1-score are all approximately 0.972, indicating that the model performs consistently well across both classes **without bias**.\nSimilarly, the micro-averaged precision, recall, and F1-score are also around 0.972. This aggregate measure, based on all individual instances, reflects the modelâ€™s overall accuracy on the test set.","metadata":{}},{"cell_type":"markdown","source":"## Test Dataset Predictions","metadata":{}},{"cell_type":"code","source":"def predict_image(model, image_path, class_names=['Cat', 'Dog'], image_size=224):\n    if not os.path.exists(image_path):\n        print(\"File not found!\")\n        return\n\n    img = cv2.imread(image_path)\n    img_rgb = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n\n    plt.imshow(img_rgb)\n    plt.axis('off')\n    plt.title(\"Input Image\")\n    plt.show()\n\n    img_resized = cv2.resize(img, (image_size, image_size))\n    img_scaled = img_resized / 255.0\n    img_input = np.expand_dims(img_scaled, axis=0)  \n\n    prediction = model.predict(img_input)[0]\n    predicted_class = np.argmax(prediction)\n    confidence = prediction[predicted_class] * 100\n\n    print(f\" Prediction: {class_names[predicted_class]} ({confidence:.2f}% confidence)\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-28T07:55:41.064966Z","iopub.execute_input":"2025-07-28T07:55:41.065336Z","iopub.status.idle":"2025-07-28T07:55:41.073788Z","shell.execute_reply.started":"2025-07-28T07:55:41.065311Z","shell.execute_reply":"2025-07-28T07:55:41.072792Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"image_path = input(\" Enter the path to the image: \")\npredict_image(model, image_path)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-28T07:55:43.349283Z","iopub.execute_input":"2025-07-28T07:55:43.34961Z","iopub.status.idle":"2025-07-28T07:55:51.259977Z","shell.execute_reply.started":"2025-07-28T07:55:43.349586Z","shell.execute_reply":"2025-07-28T07:55:51.258925Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"image_path = input(\"Enter the path to the image: \")\npredict_image(model, image_path)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-28T07:56:00.202852Z","iopub.execute_input":"2025-07-28T07:56:00.203181Z","iopub.status.idle":"2025-07-28T07:56:05.912163Z","shell.execute_reply.started":"2025-07-28T07:56:00.203159Z","shell.execute_reply":"2025-07-28T07:56:05.911233Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Model Predictions on Unseen Images","metadata":{}},{"cell_type":"code","source":" predict_image(model, image_path, class_names=['Cat', 'Dog'], image_size=224):\n    if not os.path.exists(image_path):\n        print(\"File not found:\", image_path)\n        return\n\n    img = cv2.imread(image_path)\n    if img is None:\n        print(\"Could not read image:\", image_path)\n        return\n\n    img_rgb = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n\n    plt.imshow(img_rgb)\n    plt.axis('off')\n    plt.title(f\"Input Image: {os.path.basename(image_path)}\")\n    plt.show()\n\n    img_resized = cv2.resize(img, (image_size, image_size))\n    img_scaled = img_resized / 255.0\n    img_input = np.expand_dims(img_scaled, axis=0)  \n\n    prediction = model.predict(img_input)[0]\n    predicted_class = np.argmax(prediction)\n    confidence = prediction[predicted_class] * 100\n\n    print(f\"Prediction: {class_names[predicted_class]} ({confidence:.2f}% confidence)\")\n    print(\"-\"*40)\n\nurls = [\n    \"https://rimage.gnst.jp/livejapan.com/public/article/detail/a/00/01/a0001799/img/en/a0001799_parts_5dce6175d72ef.jpg?20200605180827&q=80\",\n    \"https://rimage.gnst.jp/livejapan.com/public/article/detail/a/00/01/a0001799/img/en/a0001799_parts_5d96d0dc280c5.jpg?20200605180827&q=80\",\n    \"\"\n   \n   \n]\n\nsave_dir = \"/kaggle/working/test_images\"\nos.makedirs(save_dir, exist_ok=True)\n\n\nfor i, url in enumerate(urls):\n    ext = url.split('.')[-1].split('?')[0]\n    save_path = os.path.join(save_dir, f\"image_{i}.{ext}\")\n\n    try:\n        urllib.request.urlretrieve(url, save_path)\n        print(f\"Downloaded image {i} to {save_path}\")\n        predict_image(model, save_path)\n    except Exception as e:\n        print(f\"Failed to download or predict for {url}: {e}\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-28T07:56:13.632983Z","iopub.execute_input":"2025-07-28T07:56:13.633294Z","iopub.status.idle":"2025-07-28T07:56:15.239527Z","shell.execute_reply.started":"2025-07-28T07:56:13.633271Z","shell.execute_reply":"2025-07-28T07:56:15.23846Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## ðŸŽ¯ Conclusion","metadata":{}},{"cell_type":"markdown","source":"This project demonstrated the effectiveness of transfer learning using MobileNetV2 for binary image classification. Despite training on a relatively small subset of the dataset, the model achieved impressive results with **98% accuracy on unseen** test data. This highlights the power of pretrained models in achieving high performance with limited training resources.","metadata":{}}]}